{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/PatchTST/PatchTST_supervised\")\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import uuid\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "from models.PatchTST import Model as PatchTST\n",
    "from datetime import datetime\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration holder for time series forecasting models.\n",
    "\n",
    "    This class initializes and manages model configuration parameters,\n",
    "    allowing overrides via keyword arguments. It also determines\n",
    "    the computation device (CPU or CUDA).\n",
    "\n",
    "    Attributes:\n",
    "        seq_len (int): Length of the input sequence.\n",
    "        pred_len (int): Length of the prediction sequence.\n",
    "        enc_in (int): Number of input features.\n",
    "        device (torch.device): Computation device, automatically set to CUDA if available.\n",
    "\n",
    "    Methods:\n",
    "        default_params():\n",
    "            Returns a dictionary of default parameters.\n",
    "            Must be implemented by subclasses.\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, pred_len, enc_in, **kwargs):\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.enc_in = enc_in\n",
    "\n",
    "        defaults = self.default_params()\n",
    "        for k, v in defaults.items():\n",
    "            setattr(self, k, kwargs.get(k, v))\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def default_params(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class HourlyConfig(Config):\n",
    "    \"\"\"\n",
    "    Configuration for hourly time series forecasting tasks.\n",
    "\n",
    "    This subclass of `Config` defines default hyperparameters and model settings\n",
    "    specifically optimized for hourly-level data input.\n",
    "\n",
    "    Default parameters include settings for:\n",
    "    - Transformer model architecture (layers, heads, dimensions)\n",
    "    - Data preprocessing and input slicing (patch length, stride)\n",
    "    - Training parameters (batch size, learning rate, optimizer, scheduler)\n",
    "    - Regularization and dropout rates\n",
    "    - Optional techniques (Reversible Instance Normalization, decomposition, etc.)\n",
    "\n",
    "    Attributes inherited from Config:\n",
    "        seq_len (int): Length of the input sequence.\n",
    "        pred_len (int): Length of the prediction sequence.\n",
    "        enc_in (int): Number of input features.\n",
    "        device (torch.device): Automatically determined computing device.\n",
    "    \"\"\"\n",
    "    def default_params(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of default configuration parameters for hourly data.\n",
    "\n",
    "        Returns:\n",
    "            dict: Mapping of parameter names to their default values. Includes model\n",
    "            architecture, training setup, and normalization strategies.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'e_layers': 4, 'd_model': 256, 'n_heads': 4, 'd_ff': 128,\n",
    "            'patch_len': min(16, self.seq_len), 'stride': min(16, self.seq_len),\n",
    "            'dropout': 0.1, 'attn_dropout': 0.1, 'fc_dropout': 0.1, 'head_dropout': 0.0,\n",
    "            'kernel_size': 25, 'padding_patch': 'end',\n",
    "            'individual': False, 'head_type': 'prediction', 'revin': True, 'affine': True,\n",
    "            'subtract_last': False, 'decomposition': False,\n",
    "            'batch_size': 16, 'epochs': 4, 'lr': 1e-3, 'weight_decay': 1e-2,\n",
    "            'optimizer': 'adamw', 'scheduler': 'cosine', 'T_0': 5, 'T_mult': 2,\n",
    "            'grad_clip': 0.1, 'patience': 10, 'warmup_epochs': 0\n",
    "        }\n",
    "\n",
    "class DailyConfig(Config):\n",
    "    \"\"\"\n",
    "    Configuration for daily time series forecasting tasks.\n",
    "\n",
    "    This subclass of `Config` provides default settings tailored for\n",
    "    daily-level data frequency. It includes model architecture, training\n",
    "    parameters, and data processing options.\n",
    "\n",
    "    Default parameters emphasize:\n",
    "    - Moderate sequence segmentation (e.g., 7-day patch)\n",
    "    - Standard transformer architecture\n",
    "    - Conservative dropout settings\n",
    "    - Early stopping and learning rate warmup\n",
    "\n",
    "    Attributes inherited from Config:\n",
    "        seq_len (int): Input sequence length.\n",
    "        pred_len (int): Prediction horizon.\n",
    "        enc_in (int): Number of input features.\n",
    "        device (torch.device): CUDA if available, else CPU.\n",
    "    \"\"\"\n",
    "    def default_params(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of default configuration values specific to daily data.\n",
    "\n",
    "        Returns:\n",
    "            dict: Default parameters including model size, patching, dropout rates,\n",
    "                  training settings, and normalization strategies.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'e_layers': 4, 'd_model': 256, 'n_heads': 4, 'd_ff': 128,\n",
    "            'patch_len': min(7, self.seq_len), 'stride': min(7, self.seq_len),\n",
    "            'dropout': 0.2, 'attn_dropout': 0.1, 'fc_dropout': 0.1, 'head_dropout': 0.1,\n",
    "            'kernel_size': 7, 'padding_patch': 'end',\n",
    "            'individual': False, 'head_type': 'prediction', 'revin': True, 'affine': True,\n",
    "            'subtract_last': False, 'decomposition': False,\n",
    "            'batch_size': 16, 'epochs': 10, 'lr': 1e-3, 'weight_decay': 1e-3,\n",
    "            'optimizer': 'adamw', 'scheduler': 'cosine', 'T_0': 3, 'T_mult': 1,\n",
    "            'grad_clip': 0.1, 'patience': 5, 'warmup_epochs': 3\n",
    "        }\n",
    "\n",
    "def scale_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, StandardScaler]]:\n",
    "    \"\"\"\n",
    "    Applies standard scaling (zero mean, unit variance) to numerical columns in a DataFrame,\n",
    "    excluding known time-based cyclic features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing numeric and possibly time-related features.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, Dict[str, StandardScaler]]:\n",
    "            - Scaled DataFrame with specified columns standardized.\n",
    "            - Dictionary mapping column names to their fitted StandardScaler instances\n",
    "              for potential inverse transformation or reuse.\n",
    "\n",
    "    Notes:\n",
    "        The following time-based or cyclic features are excluded from scaling:\n",
    "        'dayofweek', 'month', 'is_weekend', 'dayofyear',\n",
    "        'dayofweek_sin', 'dayofweek_cos',\n",
    "        'month_sin', 'month_cos',\n",
    "        'dayofyear_sin', 'dayofyear_cos'.\n",
    "    \"\"\"\n",
    "    scalers = {}\n",
    "    # Time-related features to exclude from scaling\n",
    "    skip_cols = {\n",
    "        'dayofweek', 'month', 'is_weekend', 'dayofyear',\n",
    "        'dayofweek_sin', 'dayofweek_cos',\n",
    "        'month_sin', 'month_cos',\n",
    "        'dayofyear_sin', 'dayofyear_cos'\n",
    "    }\n",
    "\n",
    "    for col in df.select_dtypes('number').columns:\n",
    "        if col not in skip_cols:\n",
    "            sc = StandardScaler()\n",
    "            df[[col]] = sc.fit_transform(df[[col]])\n",
    "            scalers[col] = sc\n",
    "    return df, scalers\n",
    "\n",
    "def split_and_window(df: pd.DataFrame, seq_len, pred_len) -> Tuple:\n",
    "    \"\"\"\n",
    "    Splits a time series DataFrame into training, validation, and test sets,\n",
    "    then applies sliding window segmentation to create input-output pairs.\n",
    "\n",
    "    The function removes the 'date' column, splits the remaining data\n",
    "    60%/20%/20% into train/val/test, and generates windows of past `seq_len`\n",
    "    timesteps as inputs and future `pred_len` timesteps as targets.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with a 'date' column and numerical features.\n",
    "        seq_len (int): Length of input sequences (historical window).\n",
    "        pred_len (int): Length of prediction sequences (forecast horizon).\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            - (X_tr, Y_tr): Training input-output pairs as NumPy arrays.\n",
    "            - (X_va, Y_va): Validation input-output pairs as NumPy arrays.\n",
    "            - (X_te, Y_te): Test input-output pairs as NumPy arrays.\n",
    "            - List[str]: Names of feature columns used (excluding 'date').\n",
    "\n",
    "    Notes:\n",
    "        Data is split based on row indices, not shuffling.\n",
    "        Input and output arrays are returned with dtype float32.\n",
    "    \"\"\"\n",
    "    arr = df.drop(columns=['date']).values\n",
    "    n = len(arr)\n",
    "    t1, t2 = int(0.6 * n), int(0.8 * n)\n",
    "\n",
    "    print(f\"[Split] Total: {n}, Train: {t1}, Val: {t2 - t1}, Test: {n - t2}\")\n",
    "\n",
    "    def window(data):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(data) - seq_len - pred_len):\n",
    "            X.append(data[i:i+seq_len])\n",
    "            Y.append(data[i+seq_len:i+seq_len+pred_len])\n",
    "        return np.array(X, np.float32), np.array(Y, np.float32)\n",
    "\n",
    "    X_tr, Y_tr = window(arr[:t1])\n",
    "    X_va, Y_va = window(arr[t1:t2])\n",
    "    X_te, Y_te = window(arr[t2:])\n",
    "\n",
    "\n",
    "    print(f\"[Windowed] Train X: {X_tr.shape}, Y: {Y_tr.shape}\")\n",
    "    print(f\"[Windowed] Val   X: {X_va.shape}, Y: {Y_va.shape}\")\n",
    "    print(f\"[Windowed] Test  X: {X_te.shape}, Y: {Y_te.shape}\")\n",
    "\n",
    "    return (X_tr, Y_tr), (X_va, Y_va), (X_te, Y_te), df.drop(columns='date').columns.tolist()\n",
    "\n",
    "\n",
    "def prepare_loaders(Xs, Ys, cfg) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Converts split input-output NumPy arrays into PyTorch DataLoaders\n",
    "    for training, validation, and testing.\n",
    "\n",
    "    Args:\n",
    "        Xs (Tuple[np.ndarray, np.ndarray, np.ndarray]): Tuple containing input arrays for train, val, and test.\n",
    "        Ys (Tuple[np.ndarray, np.ndarray, np.ndarray]): Tuple containing target arrays for train, val, and test.\n",
    "        cfg (Config): Configuration object containing at least `batch_size`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "            - train_loader: DataLoader for training set (with shuffling).\n",
    "            - val_loader: DataLoader for validation set (with shuffling).\n",
    "            - test_loader: DataLoader for test set (no shuffling).\n",
    "\n",
    "    Notes:\n",
    "        Inputs and targets are converted to `torch.tensor` from NumPy arrays.\n",
    "        Only training and validation loaders have shuffling enabled.\n",
    "    \"\"\"\n",
    "    to_loader = lambda x, y: DataLoader(TensorDataset(torch.tensor(x), torch.tensor(y)), batch_size=cfg.batch_size, shuffle=True)\n",
    "    train_loader = to_loader(Xs[0], Ys[0])\n",
    "    val_loader = to_loader(Xs[1], Ys[1])\n",
    "    test_loader = DataLoader(TensorDataset(torch.tensor(Xs[2]), torch.tensor(Ys[2])), batch_size=cfg.batch_size)\n",
    "\n",
    "    print(f\"[Loaders] Train batches: {len(train_loader)}, Batch size: {cfg.batch_size}\")\n",
    "    print(f\"[Loaders] Val   batches: {len(val_loader)}\")\n",
    "    print(f\"[Loaders] Test  batches: {len(test_loader)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def compute_channel_weights_from_val(model, val_loader, device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes normalized per-channel weights based on RMSE over the validation set.\n",
    "\n",
    "    This function evaluates the model on the validation data, computes root mean\n",
    "    squared error (RMSE) per channel, and assigns higher weights to channels\n",
    "    with lower error. The weights are then normalized to have a mean of 1.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model to evaluate.\n",
    "        val_loader (DataLoader): Validation set DataLoader.\n",
    "        device (torch.device): Computation device (CPU or CUDA).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 1D tensor of shape (num_channels,) containing normalized weights.\n",
    "\n",
    "    Notes:\n",
    "        - The function disables gradient computation (`torch.no_grad()`).\n",
    "        - Channels with lower RMSE receive higher weights.\n",
    "        - `1e-6` is added to avoid division by zero.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds_val, trues_val = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            out = model(xb).cpu().numpy()\n",
    "            preds_val.append(out)\n",
    "            trues_val.append(yb.numpy())\n",
    "    preds_val = np.concatenate(preds_val, axis=0)\n",
    "    trues_val = np.concatenate(trues_val, axis=0)\n",
    "    rmse = np.sqrt(((trues_val - preds_val) ** 2).mean(axis=(0, 1)))\n",
    "    weights = 1.0 / (rmse + 1e-6)\n",
    "    return torch.tensor(weights / weights.mean(), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, opt, sched, channel_weights, cfg):\n",
    "    \"\"\"\n",
    "    Trains the model for a single epoch using Smooth L1 loss with channel-wise weighting.\n",
    "\n",
    "    This function performs forward and backward passes on the training data,\n",
    "    applies gradient clipping, updates the optimizer, and steps the learning rate scheduler.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train.\n",
    "        loader (DataLoader): DataLoader providing training batches.\n",
    "        opt (torch.optim.Optimizer): Optimizer instance (e.g., Adam, AdamW).\n",
    "        sched (torch.optim.lr_scheduler._LRScheduler): Learning rate scheduler.\n",
    "        channel_weights (torch.Tensor): 1D tensor of weights to apply to each channel's loss.\n",
    "        cfg (Config): Configuration object with `.device` and `.grad_clip`.\n",
    "\n",
    "    Returns:\n",
    "        float: Average training loss over all batches in the epoch.\n",
    "\n",
    "    Notes:\n",
    "        - Uses `Smooth L1` loss (Huber loss) with `beta=1.0`.\n",
    "        - Applies per-channel weighting and global averaging.\n",
    "        - Gradients are clipped to `cfg.grad_clip` for stability.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (xb, yb) in enumerate(loader):\n",
    "        if batch_idx == 0:\n",
    "            print(f\"[Train Batch] xb: {xb.shape}, yb: {yb.shape}\")\n",
    "        xb, yb = xb.to(cfg.device), yb.to(cfg.device)\n",
    "        opt.zero_grad()\n",
    "        loss = torch.nn.functional.smooth_l1_loss(model(xb), yb, reduction='none', beta=1.0)\n",
    "        loss = (loss * channel_weights.to(cfg.device).view(1, 1, -1)).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "        opt.step()\n",
    "        sched.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, channel_weights, cfg):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(cfg.device), yb.to(cfg.device)\n",
    "            loss = torch.nn.functional.smooth_l1_loss(model(xb), yb, reduction='none', beta=1.0)\n",
    "            loss = (loss * channel_weights.to(cfg.device).view(1, 1, -1)).mean()\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def plot_prediction_sequence(\n",
    "    raw_preds, raw_trues, columns,\n",
    "    channel_idx: int, t0: int,\n",
    "    fname: Optional[str] = None,\n",
    "    align_future: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot test prediction vs. truth (first 7 days), and future forecast (next 7 days) as a dotted line.\n",
    "    Optionally aligns the future forecast for smoother visual continuity.\n",
    "\n",
    "    Parameters:\n",
    "        raw_preds: np.ndarray, shape (T, H, C) - model predictions\n",
    "        raw_trues: np.ndarray, shape (T, H, C) - ground truth values\n",
    "        columns: List[str] - feature/column names\n",
    "        channel_idx: int - index of the channel to plot\n",
    "        t0: int - start time window index\n",
    "        fname: Optional[str] - optional file path to save the figure\n",
    "        align_future: bool - whether to visually align the forecast to test prediction\n",
    "    \"\"\"\n",
    "    H = raw_preds.shape[1]\n",
    "\n",
    "    y_pred_test = raw_preds[t0, :, channel_idx]\n",
    "    y_true_test = raw_trues[t0, :, channel_idx]\n",
    "    x_test = np.arange(0, H)\n",
    "\n",
    "    # Forecast for next horizon\n",
    "    if t0 + 1 < raw_preds.shape[0]:\n",
    "        y_pred_future = raw_preds[t0 + 1, :, channel_idx]\n",
    "        x_future = np.arange(H, H * 2)\n",
    "\n",
    "        if align_future:\n",
    "            offset = y_pred_test[-1] - y_pred_future[0]\n",
    "            y_pred_future = y_pred_future + offset\n",
    "    else:\n",
    "        y_pred_future = None\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(np.mean((y_pred_test - y_true_test) ** 2))\n",
    "    mean_val = np.mean(y_true_test)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(x_test, y_true_test, label='True (Test)', color='blue')\n",
    "    plt.plot(x_test, y_pred_test, label='Predicted (Test)', color='orange')\n",
    "    if y_pred_future is not None:\n",
    "        plt.plot(x_future, y_pred_future, label='Forecast (Future)', color='green', linestyle='dotted')\n",
    "\n",
    "    plt.title(f\"{channel_idx} — RMSE: {rmse:.2f}, Mean: {mean_val:.2f}\")\n",
    "    plt.xlabel('Time Index (Days)')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if fname:\n",
    "        plt.savefig(fname)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def filter_high_rmse_channels(raw_preds, raw_trues, columns, threshold=None):\n",
    "    if threshold is not None:\n",
    "        per_series_rmse = np.sqrt(((raw_trues - raw_preds) ** 2).mean(axis=(0, 1)))\n",
    "        bad_idx = [i for i, rmse in enumerate(per_series_rmse) if rmse > threshold]\n",
    "\n",
    "        if bad_idx:\n",
    "            print(f\"Dropping {len(bad_idx)} channels with RMSE > {threshold}\")\n",
    "            print(f\"Dropped columns: {[columns[i] for i in bad_idx]}\")\n",
    "\n",
    "            keep_idx = [i for i in range(len(columns)) if i not in bad_idx]\n",
    "            raw_preds = raw_preds[:, :, keep_idx]\n",
    "            raw_trues = raw_trues[:, :, keep_idx]\n",
    "            columns = [columns[i] for i in keep_idx]\n",
    "\n",
    "    flat_df = pd.DataFrame(raw_trues.reshape(-1, len(columns)), columns=columns)\n",
    "    print(f\"\\nFiltered dataframe shape: {flat_df.shape} (rows x columns)\")\n",
    "    print(\"Column means after filtering:\")\n",
    "    print(flat_df.mean())\n",
    "\n",
    "    mae = mean_absolute_error(raw_trues.flatten(), raw_preds.flatten())\n",
    "    rmse = np.sqrt(((raw_trues - raw_preds) ** 2).mean())\n",
    "    r2 = r2_score(raw_trues.flatten(), raw_preds.flatten())\n",
    "    smape_score = smape(raw_trues.flatten(), raw_preds.flatten())\n",
    "    print(f\"Global → MAE={mae:.3f}, RMSE={rmse:.3f}, R²={r2:.4f}, SMAPE={smape_score:.2f}%\")\n",
    "\n",
    "    return raw_preds, raw_trues, columns, mae, rmse, r2\n",
    "\n",
    "# --- New Plot Function ---\n",
    "def plot_forecast(preds, trues, feature_idxs=[2], save_path=\"/content/logs\", num_samples=3):\n",
    "    num_samples = min(num_samples, preds.shape[0])\n",
    "\n",
    "    for feature_idx in feature_idxs:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            true_series = trues[i, :, feature_idx]\n",
    "            pred_series = preds[i, :, feature_idx]\n",
    "\n",
    "            rmse = np.sqrt(np.mean((pred_series - true_series) ** 2))\n",
    "            mean_val = np.mean(true_series)\n",
    "\n",
    "            plt.subplot(num_samples, 1, i + 1)\n",
    "            plt.plot(true_series, label=\"True\")\n",
    "            plt.plot(pred_series, label=\"Pred\")\n",
    "            plt.title(f\"Sample {i} | Feature {feature_idx} | RMSE: {rmse:.2f}, Mean: {mean_val:.2f}\")\n",
    "            plt.legend(loc='upper right')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            fname = f\"{osp.splitext(save_path)[0]}_feature{feature_idx}_multi.png\"\n",
    "            plt.savefig(fname)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_scheduler_with_warmup(optimizer, total_steps, warmup_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return 0.5 * (1.0 + np.cos(np.pi * progress))\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    return np.mean(np.abs(y_pred - y_true) / np.clip(denominator, 1e-8, None)) * 100\n",
    "\n",
    "def compute_global_metrics(raw_preds, raw_trues):\n",
    "    mae = mean_absolute_error(raw_trues.flatten(), raw_preds.flatten())\n",
    "    rmse = np.sqrt(((raw_trues - raw_preds) ** 2).mean())\n",
    "    r2 = r2_score(raw_trues.flatten(), raw_preds.flatten())\n",
    "    smape_score = smape(raw_trues.flatten(), raw_preds.flatten())\n",
    "    return mae, rmse, r2, smape_score\n",
    "\n",
    "def compute_per_channel_metrics(raw_preds, raw_trues):\n",
    "    mean_true_per_channel = raw_trues.mean(axis=(0, 1))\n",
    "    rmse_per_channel = np.sqrt(((raw_preds - raw_trues) ** 2).mean(axis=(0, 1)))\n",
    "    nrmse_per_channel = np.divide(\n",
    "        rmse_per_channel,\n",
    "        np.abs(mean_true_per_channel),\n",
    "        out=np.full_like(rmse_per_channel, np.inf),\n",
    "        where=mean_true_per_channel != 0\n",
    "    )\n",
    "    return mean_true_per_channel, rmse_per_channel, nrmse_per_channel\n",
    "\n",
    "def categorize_nrmse(nrmse):\n",
    "    if nrmse < 0.1: return 'Excellent'\n",
    "    elif nrmse < 0.3: return 'Good'\n",
    "    elif nrmse < 0.5: return 'Fair'\n",
    "    elif nrmse < 1.0: return 'Poor'\n",
    "    else: return 'Bad'\n",
    "\n",
    "def generate_stats_df(columns, mean_true, rmse, nrmse):\n",
    "    categories = [categorize_nrmse(val) for val in nrmse]\n",
    "    return pd.DataFrame({\n",
    "        \"column\": columns,\n",
    "        \"mean\": mean_true,\n",
    "        \"rmse\": rmse,\n",
    "        \"nrmse\": nrmse,\n",
    "        \"category\": categories\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_and_plot(model, test_loader, scalers, columns, cfg, basename, rmse_threshold=None):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            out = model(xb.to(cfg.device)).cpu().numpy()\n",
    "            preds.append(out)\n",
    "            trues.append(yb.numpy())\n",
    "    preds, trues = np.concatenate(preds), np.concatenate(trues)\n",
    "\n",
    "    print(f\"[Evaluation] Preds shape: {preds.shape}\")\n",
    "    print(f\"[Evaluation] Trues shape: {trues.shape}\")\n",
    "\n",
    "    # --- Inverse Transform ---\n",
    "    raw_preds = np.zeros_like(preds)\n",
    "    raw_trues = np.zeros_like(trues)\n",
    "    for i, col in enumerate(columns):\n",
    "        if col in scalers:\n",
    "            raw_preds[:, :, i] = scalers[col].inverse_transform(preds[:, :, i].reshape(-1, 1)).reshape(preds.shape[0], preds.shape[1])\n",
    "            raw_trues[:, :, i] = scalers[col].inverse_transform(trues[:, :, i].reshape(-1, 1)).reshape(trues.shape[0], trues.shape[1])\n",
    "        else:\n",
    "            raw_preds[:, :, i] = preds[:, :, i]\n",
    "            raw_trues[:, :, i] = trues[:, :, i]\n",
    "\n",
    "    # --- Exclude engineered features from metrics ---\n",
    "    engineered_cols = {\n",
    "        'dayofweek', 'month', 'is_weekend', 'dayofyear',\n",
    "        'dayofweek_sin', 'dayofweek_cos',\n",
    "        'month_sin', 'month_cos',\n",
    "        'dayofyear_sin', 'dayofyear_cos'\n",
    "    }\n",
    "\n",
    "    metric_mask = [i for i, col in enumerate(columns) if col not in engineered_cols]\n",
    "    raw_preds_metrics = raw_preds[:, :, metric_mask]\n",
    "    raw_trues_metrics = raw_trues[:, :, metric_mask]\n",
    "    metric_columns = [columns[i] for i in metric_mask]\n",
    "\n",
    "    # --- Compute global metrics on unfiltered + unengineered channels ---\n",
    "    mae, rmse, r2, smape_score = compute_global_metrics(raw_preds_metrics, raw_trues_metrics)\n",
    "    print(f\"[Global Metrics] MAE={mae:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}, SMAPE={smape_score:.2f}%\")\n",
    "\n",
    "    # --- Optional filtering of high-RMSE channels (only for visualization and per-channel) ---\n",
    "    filtered_preds, filtered_trues, filtered_columns = raw_preds_metrics, raw_trues_metrics, metric_columns\n",
    "    if rmse_threshold is not None:\n",
    "        filtered_preds, filtered_trues, filtered_columns = filter_high_rmse_channels(\n",
    "            raw_preds_metrics, raw_trues_metrics, metric_columns, threshold=rmse_threshold\n",
    "        )\n",
    "\n",
    "    # --- Per-channel metrics ---\n",
    "    mean_vals, rmse_vals, nrmse_vals = compute_per_channel_metrics(filtered_preds, filtered_trues)\n",
    "    stats_df = generate_stats_df(filtered_columns, mean_vals, rmse_vals, nrmse_vals)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    stats_df.to_csv(f\"logs/{basename}_per_channel_stats_{timestamp}.csv\", index=False)\n",
    "\n",
    "    print(\"Mean RMSE (filtered):\", stats_df['rmse'].mean())\n",
    "    print(\"Mean NRMSE (filtered):\", stats_df['nrmse'].mean())\n",
    "    print(\"Summary by Category:\")\n",
    "    print(stats_df['category'].value_counts())\n",
    "\n",
    "    for cat in ['Poor', 'Fair', 'Bad']:\n",
    "        filtered = stats_df[stats_df['category'] == cat]\n",
    "        if not filtered.empty:\n",
    "            print(f\"\\n{cat} Channels:\")\n",
    "            print(filtered[['column', 'mean', 'rmse', 'nrmse']].sort_values(by='nrmse').to_string(index=False))\n",
    "\n",
    "    # --- Plots ---\n",
    "    t0 = min(100, raw_preds.shape[0] - 1)\n",
    "    plot_prediction_sequence(\n",
    "    raw_preds, raw_trues, columns,\n",
    "    channel_idx=1, t0=8,\n",
    "    align_future=True\n",
    "    )\n",
    "\n",
    "    find_best_prediction_plots(raw_preds_metrics, raw_trues_metrics, metric_columns)\n",
    "\n",
    "    #plot_forecast(raw_preds, raw_trues, feature_idxs=[1,2], save_path=f\"/content/logs/{basename}_forecast{timestamp}.png\")\n",
    "    return raw_preds, raw_trues, columns, {\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'smape': smape_score,\n",
    "        'nrmse_per_channel': nrmse_vals.tolist(),\n",
    "        'columns': filtered_columns\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def test_loss(model, loader, channel_weights, cfg):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(cfg.device), yb.to(cfg.device)\n",
    "            loss = torch.nn.functional.smooth_l1_loss(model(xb), yb, reduction='none', beta=1.0)\n",
    "            loss = (loss * channel_weights.to(cfg.device).view(1, 1, -1)).mean()\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# --- Training ---\n",
    "def run_experiment(csv_path, seq_len, pred_len, config_class):\n",
    "    basename = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df, scalers = scale_data(df)\n",
    "    (X_tr, Y_tr), (X_va, Y_va), (X_te, Y_te), columns = split_and_window(df, seq_len, pred_len)\n",
    "    cfg = config_class(seq_len, pred_len, enc_in=X_tr.shape[-1])\n",
    "    train_loader, val_loader, test_loader = prepare_loaders((X_tr, X_va, X_te), (Y_tr, Y_va, Y_te), cfg)\n",
    "\n",
    "    model = PatchTST(cfg).to(cfg.device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    total_steps = cfg.epochs * len(train_loader)\n",
    "    warmup_steps = cfg.warmup_epochs * len(train_loader)\n",
    "    sched = create_scheduler_with_warmup(opt, total_steps, warmup_steps)\n",
    "\n",
    "    channel_weights = torch.ones(cfg.enc_in)\n",
    "    best_val, trigger = float('inf'), 0\n",
    "    train_losses, val_losses = [] ,[]\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        if epoch == 1:\n",
    "            channel_weights = compute_channel_weights_from_val(model, val_loader, cfg.device)\n",
    "        train_loss = train_one_epoch(model, train_loader, opt, sched, channel_weights, cfg)\n",
    "        val_loss = validate(model, val_loader, channel_weights, cfg)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"[PatchTST][{basename}] Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        if val_loss < best_val:\n",
    "            best_val, trigger = val_loss, 0\n",
    "        else:\n",
    "            trigger += 1\n",
    "            if trigger >= cfg.patience:\n",
    "                print(f\"[EarlyStopping] Epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    test_l = test_loss(model, test_loader, channel_weights, cfg)\n",
    "    print(f\"[PatchTST][{basename}] Final Test Loss: {test_l:.4f}\")\n",
    "    raw_preds, raw_trues, columns, result = evaluate_and_plot(model, test_loader, scalers, columns, cfg, basename)\n",
    "    np.savez_compressed(f\"logs/patchtst_outputs_{timestamp}_{basename}.npz\",\n",
    "                    preds=raw_preds, trues=raw_trues, columns=np.array(columns))\n",
    "    result['test_loss'] = test_l\n",
    "    print(f\"[PatchTST][{basename}] Final Test Loss: {test_l:.4f}\")\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- COLAB FALLBACK ---\n",
    "    csv_path_for_hourly = \"/content/PatchTST/PatchTST_supervised/Dataset/electricity_hourly_transformed_3.csv\"\n",
    "    csv_path_for_daily = \"/content/PatchTST/PatchTST_supervised/Dataset/electricity_daily_transformed_5.csv\"\n",
    "    config_type ='daily'  # Change to 'daily' if needed\n",
    "    if config_type=='hourly':\n",
    "      seq_len = 512\n",
    "      pred_len = 24\n",
    "    elif config_type=='daily':\n",
    "      seq_len = 120\n",
    "      pred_len = 7\n",
    "    config_class = HourlyConfig if config_type == 'hourly' else DailyConfig\n",
    "    if config_type=='hourly':\n",
    "      result = run_experiment(csv_path_for_hourly, seq_len, pred_len, config_class)\n",
    "      pd.DataFrame([result]).to_csv(f\"logs/combined_results_{timestamp}.csv\", index=False)\n",
    "    elif config_type=='daily':\n",
    "      result = run_experiment(csv_path_for_daily, seq_len, pred_len, config_class)\n",
    "      pd.DataFrame([result]).to_csv(f\"logs/combined_results_{timestamp}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
