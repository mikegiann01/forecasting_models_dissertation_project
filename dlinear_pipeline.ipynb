{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4047a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlinear_pipeline.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from typing import Tuple, Dict, Optional, List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as osp\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "\n",
    "class DatasetType(str, Enum):\n",
    "    HOURLY = \"hourly\"\n",
    "    DAILY = \"daily\"\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    dataset: str = \"daily\"\n",
    "    data_path: str = None\n",
    "    lookback: int = None\n",
    "    horizon: int = None\n",
    "    epochs: int = 20\n",
    "    lr: float = 1e-3\n",
    "    batch_size: int = 32\n",
    "    output_dir: str = \"logs\"\n",
    "    timestamp: str = field(default_factory=lambda: datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    threshold: int = None  # Optional\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset == \"hourly\":\n",
    "            self.data_path = self.data_path or \"/content/PatchTST/PatchTST_supervised/Dataset/electricity_hourly_transformed_5.csv\"\n",
    "            self.lookback = self.lookback or 256\n",
    "            self.horizon = self.horizon or 24\n",
    "            self.threshold = self.threshold\n",
    "        elif self.dataset == \"daily\":\n",
    "            self.data_path = self.data_path or \"/content/PatchTST/PatchTST_supervised/Dataset/electricity_daily_transformed_5.csv\"\n",
    "            self.lookback = self.lookback or 120\n",
    "            self.horizon = self.horizon or 7\n",
    "            self.threshold = self.threshold\n",
    "\n",
    "        self.basename = f\"{osp.splitext(osp.basename(self.data_path))[0]}_{self.timestamp}\"\n",
    "        self.log_path = osp.join(self.output_dir, f\"{self.basename}_train_log.txt\")\n",
    "        self.metrics_path = osp.join(self.output_dir, f\"{self.basename}_metrics.txt\")\n",
    "        self.plot_path = osp.join(self.output_dir, f\"{self.basename}_forecast.png\")\n",
    "        self.preds_path = osp.join(self.output_dir, f\"{self.basename}_preds.npy\")\n",
    "        self.trues_path = osp.join(self.output_dir, f\"{self.basename}_trues.npy\")\n",
    "\n",
    "\n",
    "\n",
    "# Dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback, horizon):\n",
    "        self.X, self.y = [], []\n",
    "        for i in range(len(data) - lookback - horizon + 1):\n",
    "            self.X.append(data[i:i + lookback])\n",
    "            self.y.append(data[i + lookback:i + lookback + horizon])\n",
    "        self.X = torch.tensor(np.array(self.X), dtype=torch.float32)\n",
    "        self.y = torch.tensor(np.array(self.y), dtype=torch.float32)\n",
    "        print(f\"[Dataset Init] X: {self.X.shape}, Y: {self.y.shape}\")\n",
    "\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class DLinear(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, input_dim, hidden_dim=64):\n",
    "        super(DLinear, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "        nn.Linear(seq_len, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(hidden_dim, pred_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (B, F, T)\n",
    "        out = self.model(x)     # Apply per feature\n",
    "        return out.permute(0, 2, 1)  # (B, T, F)\n",
    "\n",
    "\n",
    "# Scaling\n",
    "def scale_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict[str, StandardScaler], list]:\n",
    "    scalers = {}\n",
    "    skip_cols = {\n",
    "        'dayofweek', 'month', 'is_weekend', 'dayofyear',\n",
    "        'dayofweek_sin', 'dayofweek_cos',\n",
    "        'month_sin', 'month_cos',\n",
    "        'dayofyear_sin', 'dayofyear_cos'\n",
    "    }\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    scaled_columns = []\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if col not in skip_cols:\n",
    "            sc = StandardScaler()\n",
    "            df[[col]] = sc.fit_transform(df[[col]])\n",
    "            scalers[col] = sc\n",
    "            scaled_columns.append(col)\n",
    "\n",
    "    return df[numeric_cols].astype(np.float32).values, scalers, scaled_columns\n",
    "\n",
    "\n",
    "# Utils\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df.sort_values(\"date\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_data(data, lookback, horizon):\n",
    "    train_size = int(len(data) * 0.6)\n",
    "    val_size = int(len(data) * 0.2)\n",
    "    train = data[:train_size]\n",
    "    val = data[train_size:train_size + val_size]\n",
    "    test = data[train_size + val_size:]\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def create_dataloaders(train, val, test, lookback, horizon, batch_size):\n",
    "    train_ds = TimeSeriesDataset(train, lookback, horizon)\n",
    "    val_ds = TimeSeriesDataset(val, lookback, horizon)\n",
    "    test_ds = TimeSeriesDataset(test, lookback, horizon)\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(val_ds, batch_size=batch_size),\n",
    "        DataLoader(test_ds, batch_size=batch_size),\n",
    "    )\n",
    "\n",
    "def compute_channel_weights_from_val(model, val_loader, device, threshold=1.0) -> torch.Tensor:\n",
    "    model.eval()\n",
    "    preds_val, trues_val = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            out = model(xb).cpu().numpy()\n",
    "            preds_val.append(out)\n",
    "            trues_val.append(yb.cpu().numpy())\n",
    "\n",
    "    preds_val = np.concatenate(preds_val, axis=0)\n",
    "    trues_val = np.concatenate(trues_val, axis=0)\n",
    "\n",
    "    rmse = np.sqrt(np.mean((trues_val - preds_val) ** 2, axis=(0, 1)))\n",
    "    mask = rmse < threshold\n",
    "    weights = np.zeros_like(rmse)\n",
    "    weights[mask] = 1.0 / (rmse[mask] + 1e-6)\n",
    "\n",
    "    if weights[mask].mean() > 0:\n",
    "        weights[mask] /= weights[mask].mean()\n",
    "\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "def weighted_mse_loss(pred, target, weights):\n",
    "    return ((weights * (pred - target) ** 2).mean())\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) + 1e-8\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / denominator)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=20, lr=1e-3, log_path=\"train_log.txt\", device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    weights = None\n",
    "\n",
    "    with open(log_path, \"w\") as f:\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_loss_total = 0\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(x_batch)\n",
    "\n",
    "                if weights is None:\n",
    "                    loss = nn.functional.mse_loss(output, y_batch)\n",
    "                else:\n",
    "                    loss = weighted_mse_loss(output, y_batch, weights.to(device))\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss_total += loss.item()\n",
    "\n",
    "            train_loss_avg = train_loss_total / len(train_loader)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss_total = 0\n",
    "            with torch.no_grad():\n",
    "                for x_val, y_val in val_loader:\n",
    "                    x_val = x_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    output = model(x_val)\n",
    "                    if weights is None:\n",
    "                        val_loss = nn.functional.mse_loss(output, y_val)\n",
    "                    else:\n",
    "                        val_loss = weighted_mse_loss(output, y_val, weights.to(device))\n",
    "                    val_loss_total += val_loss.item()\n",
    "\n",
    "            val_loss_avg = val_loss_total / len(val_loader)\n",
    "\n",
    "            log_msg = f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss_avg:.4f} | Val Loss: {val_loss_avg:.4f}\"\n",
    "            print(log_msg)\n",
    "            f.write(log_msg + \"\\n\")\n",
    "\n",
    "            weights = compute_channel_weights_from_val(model, val_loader, device)\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, result_dir=\"results\", basename=\"output\", scalers=None, scaled_columns=None):\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            pred = model(x)\n",
    "            preds.append(pred.numpy())\n",
    "            trues.append(y.numpy())\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    trues = np.concatenate(trues, axis=0)\n",
    "\n",
    "    B, T, F = preds.shape\n",
    "    preds_2d = preds.reshape(B, T, F)\n",
    "    trues_2d = trues.reshape(B, T, F)\n",
    "\n",
    "    # Inverse transform each feature\n",
    "    if scalers and scaled_columns:\n",
    "        for i, col in enumerate(scaled_columns):\n",
    "            scaler = scalers[col]\n",
    "            preds_2d[:, :, i] = scaler.inverse_transform(\n",
    "                preds_2d[:, :, i].reshape(-1, 1)\n",
    "            ).reshape(preds_2d.shape[0], preds_2d.shape[1])\n",
    "            trues_2d[:, :, i] = scaler.inverse_transform(\n",
    "                trues_2d[:, :, i].reshape(-1, 1)\n",
    "            ).reshape(trues_2d.shape[0], trues_2d.shape[1])\n",
    "\n",
    "    # Metric calculations\n",
    "    mse = np.mean((preds_2d - trues_2d) ** 2)\n",
    "    mae = np.mean(np.abs(preds_2d - trues_2d))\n",
    "    rmse = np.sqrt(mse)\n",
    "    smape_score = smape(trues_2d, preds_2d)\n",
    "\n",
    "    valid_idx = [\n",
    "    i for i in range(len(scaled_columns))\n",
    "    if not np.isclose(np.std(trues_2d[:, :, i]), 0)\n",
    "    ]\n",
    "    r2 = r2_score(\n",
    "        trues_2d[:, :, valid_idx].reshape(-1, len(valid_idx)),\n",
    "        preds_2d[:, :, valid_idx].reshape(-1, len(valid_idx))\n",
    "    )\n",
    "\n",
    "    # Save metrics and arrays\n",
    "    with open(os.path.join(result_dir, f\"{basename}_metrics.txt\"), \"w\") as f:\n",
    "        f.write(f\"MAE: {mae:.4f}\\nRMSE: {rmse:.4f}\\nR2: {r2:.4f}\\nSMAPE:{smape_score:.4f}\")\n",
    "\n",
    "    print(f\"MAE: {mae:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}, SMAPE: {smape_score:.2f}%\")\n",
    "\n",
    "    np.save(os.path.join(result_dir, f\"{basename}_preds.npy\"), preds_2d)\n",
    "    np.save(os.path.join(result_dir, f\"{basename}_trues.npy\"), trues_2d)\n",
    "\n",
    "    return preds_2d, trues_2d\n",
    "\n",
    "def plot_random_sample_forecasts(preds, trues, feature_idx=6, num_samples=3, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot a few random samples from the batch for visual clarity.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    B, T, F = preds.shape\n",
    "    indices = random.sample(range(B), min(num_samples, B))\n",
    "\n",
    "    plt.figure(figsize=(14, num_samples * 2.5))\n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(num_samples, 1, i + 1)\n",
    "        plt.plot(trues[idx, :, feature_idx], label=\"True\")\n",
    "        plt.plot(preds[idx, :, feature_idx], label=\"Pred\")\n",
    "        plt.title(f\"Random Sample {idx} - Feature {feature_idx}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_prediction_sequence(\n",
    "    raw_preds, raw_trues, columns,\n",
    "    channel_idx: int, t0: int,\n",
    "    fname: Optional[str] = f\"/content/logs/forecast.png\",\n",
    "    align_future: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot test prediction vs. truth (first 7 days), and future forecast (next 7 days) as a dotted line.\n",
    "    Optionally aligns the future forecast for smoother visual continuity.\n",
    "    \"\"\"\n",
    "    H = raw_preds.shape[1]\n",
    "\n",
    "    y_pred_test = raw_preds[t0, :, channel_idx]\n",
    "    y_true_test = raw_trues[t0, :, channel_idx]\n",
    "    x_test = np.arange(0, H)\n",
    "\n",
    "    if t0 + 1 < raw_preds.shape[0]:\n",
    "        y_pred_future = raw_preds[t0 + 1, :, channel_idx]\n",
    "        x_future = np.arange(H, H * 2)\n",
    "        if align_future:\n",
    "            offset = y_pred_test[-1] - y_pred_future[0]\n",
    "            y_pred_future = y_pred_future + offset\n",
    "    else:\n",
    "        y_pred_future = None\n",
    "\n",
    "    rmse = np.sqrt(np.mean((y_pred_test - y_true_test) ** 2))\n",
    "    mean_val = np.mean(y_true_test)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(x_test, y_true_test, label='True (Test)', color='blue')\n",
    "    plt.plot(x_test, y_pred_test, label='Predicted (Test)', color='orange')\n",
    "    if y_pred_future is not None:\n",
    "        plt.plot(x_future, y_pred_future, label='Forecast (Future)', color='green', linestyle='dotted')\n",
    "\n",
    "    plt.title(f\"{channel_idx} — RMSE: {rmse:.2f}, Mean: {mean_val:.2f}\")\n",
    "    plt.xlabel('Time Index (Days)')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if fname:\n",
    "        plt.savefig(fname)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def filter_high_rmse_channels(raw_preds, raw_trues, columns, threshold=2500, debug=False):\n",
    "    per_series_rmse = np.sqrt(((raw_trues - raw_preds) ** 2).mean(axis=(0, 1)))\n",
    "    mean_rmse = np.mean(per_series_rmse)\n",
    "    print(f\"\\nMean per-channel RMSE: {mean_rmse:.4f}\")\n",
    "\n",
    "    if threshold is None:\n",
    "        print(\"No threshold provided. Skipping channel filtering.\")\n",
    "        mae = mean_absolute_error(raw_trues.flatten(), raw_preds.flatten())\n",
    "        rmse = np.sqrt(((raw_trues - raw_preds) ** 2).mean())\n",
    "        r2 = r2_score(raw_trues.flatten(), raw_preds.flatten())\n",
    "        return raw_preds, raw_trues, columns, mae, rmse, r2\n",
    "\n",
    "    bad_idx = [i for i, rmse in enumerate(per_series_rmse) if rmse > threshold]\n",
    "\n",
    "    if bad_idx:\n",
    "        print(f\"Dropping {len(bad_idx)} channels with RMSE > {threshold}\")\n",
    "        print(f\"Dropped columns: {[columns[i] for i in bad_idx]}\")\n",
    "\n",
    "    keep_idx = [i for i in range(len(columns)) if i not in bad_idx]\n",
    "    raw_preds = raw_preds[:, :, keep_idx]\n",
    "    raw_trues = raw_trues[:, :, keep_idx]\n",
    "    columns = [columns[i] for i in keep_idx]\n",
    "    per_series_rmse = per_series_rmse[keep_idx]  # filtered\n",
    "\n",
    "    if debug:\n",
    "        print(f\"raw_preds shape: {raw_preds.shape}\")\n",
    "        print(f\"raw_trues shape: {raw_trues.shape}\")\n",
    "        print(f\"Filtered columns count: {len(columns)}\")\n",
    "\n",
    "    try:\n",
    "        flat_df = pd.DataFrame(raw_trues.reshape(-1, len(columns)), columns=columns)\n",
    "        print(f\"\\nFiltered dataframe shape: {flat_df.shape} (rows x columns)\")\n",
    "        print(\"Column means after filtering:\")\n",
    "        print(flat_df.mean())\n",
    "    except ValueError as e:\n",
    "        print(f\"Reshape failed: {e}\")\n",
    "        return raw_preds, raw_trues, columns, None, None, None\n",
    "\n",
    "    mae = mean_absolute_error(raw_trues.flatten(), raw_preds.flatten())\n",
    "    rmse = np.sqrt(((raw_trues - raw_preds) ** 2).mean())\n",
    "    r2 = r2_score(raw_trues.flatten(), raw_preds.flatten())\n",
    "    print(f\"Filtered → MAE={mae:.3f}, RMSE={rmse:.3f}, R²={r2:.4f}\")\n",
    "\n",
    "    rmse_df = pd.DataFrame({\n",
    "        \"column\": columns,\n",
    "        \"rmse\": per_series_rmse\n",
    "    })\n",
    "    rmse_csv_path = os.path.join(\"logs\", f\"{osp.splitext(config.basename)[0]}_rmse_per_channel.csv\")\n",
    "    rmse_df.to_csv(rmse_csv_path, index=False)\n",
    "    print(f\"Saved per-channel RMSE to: {rmse_csv_path}\")\n",
    "\n",
    "    return raw_preds, raw_trues, columns, mae, rmse, r2\n",
    "\n",
    "def compute_nrmse_per_series(raw_preds: np.ndarray, raw_trues: np.ndarray, columns: list) -> pd.DataFrame:\n",
    "    assert raw_preds.shape == raw_trues.shape, \"Prediction and ground truth shapes must match\"\n",
    "\n",
    "    rmse_list = []\n",
    "    mean_list = []\n",
    "    nrmse_list = []\n",
    "    category_list = []\n",
    "\n",
    "    for i in range(raw_preds.shape[2]):\n",
    "        y_true = raw_trues[:, :, i]\n",
    "        y_pred = raw_preds[:, :, i]\n",
    "\n",
    "        rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "        mean = np.mean(y_true)\n",
    "        nrmse = rmse / (mean + 1e-8)\n",
    "\n",
    "        if nrmse < 0.1:\n",
    "            category = \"Excellent\"\n",
    "        elif nrmse < 0.2:\n",
    "            category = \"Good\"\n",
    "        elif nrmse < 0.3:\n",
    "            category = \"Fair\"\n",
    "        else:\n",
    "            category = \"Poor\"\n",
    "\n",
    "        rmse_list.append(rmse)\n",
    "        mean_list.append(mean)\n",
    "        nrmse_list.append(nrmse)\n",
    "        category_list.append(category)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"column\": columns,\n",
    "        \"rmse\": rmse_list,\n",
    "        \"mean\": mean_list,\n",
    "        \"nrmse\": nrmse_list,\n",
    "        \"category\": category_list\n",
    "    })\n",
    "\n",
    "    return df.sort_values(\"nrmse\")\n",
    "\n",
    "\n",
    "# Entry point\n",
    "if __name__ == '__main__':\n",
    "    base_config = Config()\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data', type=str)\n",
    "    parser.add_argument('--lookback', type=int)\n",
    "    parser.add_argument('--horizon', type=int)\n",
    "    parser.add_argument('--epochs', type=int)\n",
    "    parser.add_argument('--lr', type=float)\n",
    "    parser.add_argument('--batch_size', type=int)\n",
    "    parser.add_argument('--output', type=str)\n",
    "    parser.add_argument('--rmse_threshold', type=float)\n",
    "    import sys\n",
    "    args = parser.parse_args([] if \"__file__\" not in globals() else sys.argv[1:])\n",
    "    for key, value in vars(args).items():\n",
    "        if value is not None and hasattr(base_config, key if key != 'data' else 'data_path'):\n",
    "            setattr(base_config, key if key != 'data' else 'data_path', value)\n",
    "\n",
    "    config = base_config\n",
    "    config.__post_init__()\n",
    "\n",
    "    df = load_data(config.data_path)\n",
    "    print(f\"[Raw DF] Shape: {df.shape}\")\n",
    "\n",
    "    data, scalers, scaled_columns = scale_data(df)\n",
    "    print(f\"[Scaled Data] Shape: {data.shape}\")\n",
    "\n",
    "    train_data, val_data, test_data = split_data(data, config.lookback, config.horizon)\n",
    "    print(f\"[Split Data] Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        train_data, val_data, test_data,\n",
    "        config.lookback, config.horizon, config.batch_size\n",
    "    )\n",
    "\n",
    "    model = DLinear(\n",
    "    seq_len=config.lookback,\n",
    "    pred_len=config.horizon,\n",
    "    input_dim=data.shape[1],\n",
    "    hidden_dim=128  # or higher, e.g., 128, 256\n",
    "    )\n",
    "\n",
    "    train_model(model, train_loader, val_loader, epochs=config.epochs, lr=config.lr, log_path=config.log_path)\n",
    "\n",
    "    preds, trues = evaluate_model(model, test_loader, result_dir=config.output_dir, basename=config.basename,\n",
    "                                  scalers=scalers, scaled_columns=scaled_columns)\n",
    "    print(f\"[Eval Output] preds: {preds.shape}, trues: {trues.shape}\")\n",
    "\n",
    "    preds, trues, filtered_columns, mae, rmse, r2 = filter_high_rmse_channels(\n",
    "        preds, trues, scaled_columns, threshold=config.threshold, debug=True\n",
    "    )\n",
    "    # Ensure columns align with pred/trues third dimension\n",
    "    aligned_columns = filtered_columns\n",
    "    if preds.shape[2] != len(filtered_columns):\n",
    "        print(f\"[Warning] Column count mismatch: {preds.shape[2]} ≠ {len(filtered_columns)}. Using fallback.\")\n",
    "        if len(scaled_columns) != preds.shape[2]:\n",
    "            print(f\"[Fixing] scaled_columns length ({len(scaled_columns)}) ≠ preds.shape[2] ({preds.shape[2]})\")\n",
    "            aligned_columns = [f\"feat_{i}\" for i in range(preds.shape[2])]\n",
    "        else:\n",
    "            aligned_columns = scaled_columns\n",
    "\n",
    "    nrmse_df = compute_nrmse_per_series(preds, trues, aligned_columns)\n",
    "    print(\"\\nPer-Series NRMSE Summary:\")\n",
    "    print(nrmse_df.to_string(index=False))\n",
    "\n",
    "    nrmse_report_path = os.path.join(config.output_dir, f\"{config.basename}_nrmse_report.csv\")\n",
    "    nrmse_df.to_csv(nrmse_report_path, index=False)\n",
    "    print(f\"\\nNRMSE report saved to: {nrmse_report_path}\")\n",
    "\n",
    "    #plot_forecast(preds, trues, feature_idxs=list(range(min(3, len(filtered_columns)))), save_path=config.plot_path)\n",
    "\n",
    "    plot_prediction_sequence(\n",
    "    raw_preds=preds,\n",
    "    raw_trues=trues,\n",
    "    columns=aligned_columns,\n",
    "    channel_idx=1,\n",
    "    t0=4,\n",
    "    fname=\"logs/sample_forecast_seq.png\",\n",
    "    align_future=True\n",
    ")\n",
    "\n",
    "    top_k_scores = find_best_prediction_plots(\n",
    "    raw_preds=preds,\n",
    "    raw_trues=trues,\n",
    "    columns=aligned_columns,\n",
    "    save_dir=os.path.join(config.output_dir, \"best_plots\"),\n",
    "    top_k=5,\n",
    "    align_future=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
